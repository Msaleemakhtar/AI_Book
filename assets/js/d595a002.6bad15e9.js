"use strict";(self.webpackChunkphysical_ai_textbook=self.webpackChunkphysical_ai_textbook||[]).push([[493],{1999:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>r,contentTitle:()=>s,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4-vla/week-12-locomotion","title":"Week 12: Locomotion and Vision-Language-Action Models","description":"Learning Objectives","source":"@site/docs/module-4-vla/week-12-locomotion.md","sourceDirName":"module-4-vla","slug":"/module-4-vla/week-12-locomotion","permalink":"/AI_Book/docs/module-4-vla/week-12-locomotion","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":12,"frontMatter":{"sidebar_position":12,"title":"Week 12: Locomotion and Vision-Language-Action Models"},"sidebar":"tutorialSidebar","previous":{"title":"Week 11: Whole-Body Humanoid Control","permalink":"/AI_Book/docs/module-4-vla/week-11-humanoids"},"next":{"title":"Week 13: Capstone Project","permalink":"/AI_Book/docs/module-4-vla/week-13-capstone"}}');var t=o(4848),a=o(8453);const l={sidebar_position:12,title:"Week 12: Locomotion and Vision-Language-Action Models"},s="Week 12: Locomotion and Vision-Language-Action Models",r={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Key Concepts",id:"key-concepts",level:2},{value:"Trajectory Optimization for Walking",id:"trajectory-optimization-for-walking",level:3},{value:"Vision-Language Models (VLMs)",id:"vision-language-models-vlms",level:3},{value:"VLA Architectures",id:"vla-architectures",level:3},{value:"Language-Conditioned Policies",id:"language-conditioned-policies",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"week-12-locomotion-and-vision-language-action-models",children:"Week 12: Locomotion and Vision-Language-Action Models"})}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Generate walking trajectories using trajectory optimization"}),"\n",(0,t.jsx)(n.li,{children:"Understand vision-language models (VLMs) for robot perception"}),"\n",(0,t.jsx)(n.li,{children:"Explore Vision-Language-Action (VLA) architectures"}),"\n",(0,t.jsx)(n.li,{children:"Integrate language commands with low-level control"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"introduction",children:"Introduction"}),"\n",(0,t.jsx)(n.p,{children:"Modern humanoid robots combine classical control (for stable walking) with learned policies (for high-level reasoning). Vision-Language-Action (VLA) models like RT-2, PaLM-E, and OpenVLA use transformer architectures to map camera images and natural language instructions directly to robot actions."}),"\n",(0,t.jsx)(n.p,{children:'This week bridges low-level locomotion with high-level VLA intelligence. You\'ll learn to generate walking gaits, then explore how VLAs enable robots to follow instructions like "walk to the red box and pick it up."'}),"\n",(0,t.jsx)(n.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,t.jsx)(n.h3,{id:"trajectory-optimization-for-walking",children:"Trajectory Optimization for Walking"}),"\n",(0,t.jsx)(n.p,{children:"Walking controllers compute footstep plans and CoM trajectories. Methods include: template models (LIPM, SLIP), trajectory optimization (direct collocation, CHOMP), and learning (RL, imitation learning)."}),"\n",(0,t.jsx)(n.h3,{id:"vision-language-models-vlms",children:"Vision-Language Models (VLMs)"}),"\n",(0,t.jsx)(n.p,{children:"VLMs like CLIP, LLaVA, and GPT-4V encode images and text into shared embeddings. They enable zero-shot object recognition, scene understanding, and visual question answering."}),"\n",(0,t.jsx)(n.h3,{id:"vla-architectures",children:"VLA Architectures"}),"\n",(0,t.jsx)(n.p,{children:"VLA models extend VLMs with action prediction heads. RT-2 (Robotics Transformer 2) fine-tunes a VLM (PaLI-X) on robot demonstration data, achieving generalization to novel objects and tasks."}),"\n",(0,t.jsx)(n.h3,{id:"language-conditioned-policies",children:"Language-Conditioned Policies"}),"\n",(0,t.jsx)(n.p,{children:"VLAs take as input: image observations, proprioceptive state, and language command. They output low-level actions (joint positions/velocities) or high-level waypoints. The control hierarchy ensures safe, stable execution."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsx)(n.p,{children:"Week 13 is the capstone - you'll build an end-to-end system combining ROS 2, Isaac Sim, and a VLA model."})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>l,x:()=>s});var i=o(6540);const t={},a=i.createContext(t);function l(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),i.createElement(a.Provider,{value:n},e.children)}}}]);